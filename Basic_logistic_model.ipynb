{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_logistic_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j4sgOk6yrzI",
        "colab_type": "code",
        "outputId": "2d01a9d2-688b-4738-ce97-5e5f9eeb3dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daPyS4jh0aHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvi import ViTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from underthesea import word_tokenize\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/My Drive/analyze_comment_fb/data/\"\n",
        "path_file = os.listdir(path)\n",
        "# check html in raw text file\n",
        "\n",
        "# j = 0\n",
        "# for i in path_file:\n",
        "# \tj+=1\n",
        "# \tprint(j)\n",
        "# \tp = path+i\n",
        "# \twith open(p,\"r\") as f:\n",
        "# \t\tdata = f.read()\n",
        "# \t\tdata = data.split('\\n')[1:]\n",
        "# \t\tdata = ''.join(str(e) for e in data)\n",
        "# \t\tif bool(BeautifulSoup(data, \"html.parser\").find()):\n",
        "# \t\t\tprint(p)\n",
        "\n",
        "#remove stopwords\n",
        "def remove_stopwords():\n",
        "\twith open('/content/drive/My Drive/analyze_comment_fb/vietnamese-stopwords.txt', 'r') as f:\n",
        "\t\tlist_stopwords = f.read().split('\\n')\n",
        "\tcnt = 0\n",
        "\tfor i in path_file:\n",
        "\t\tcnt += 1\n",
        "\t\tp = path + i\n",
        "\t\twith open(p, \"r\") as f:\n",
        "\t\t\tdata = f.read()\n",
        "\t\t\tlink, data = data.split('\\n')[0], data.split('\\n')[1:]\n",
        "\t\t\tdata = ' '.join(str(e) for e in data)\n",
        "\t\t\tdata = data.split(' ')\n",
        "\t\t\ttemp = []\n",
        "\t\t\tfor w in data:\n",
        "\t\t\t\tif w not in list_stopwords:\n",
        "\t\t\t\t\ttemp.append(w)\n",
        "\t\t\tdata = ' '.join(str(e) for e in temp)\n",
        "\t\t\tpreprocessing_file = '/content/drive/My Drive/analyze_comment_fb/preprocessing/' + i\n",
        "\t\t\twith open(preprocessing_file, 'w') as write:\n",
        "\t\t\t\twrite.write(link + '\\n')\n",
        "\t\t\t\twrite.write(data)\n",
        "\t\t\tprint('Saved new file' + preprocessing_file + ' after preprocessing !')\n",
        "\tprint(\"No of saved doc : \" + str(cnt))\n",
        "\n",
        "#Tokenize\n",
        "def tokenize(doc):\n",
        "\twith open(doc,'r') as f:\n",
        "\t\tdata = f.read()\n",
        "\t\tlink,data = data.split('\\n')[0],data.split('\\n')[1:]\n",
        "\t\tdata = ' '.join(str(e) for e in data)\n",
        "\t\treturn word_tokenize(data)\t\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByTSDmKj0b3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_stopwords()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Fm9lXubM5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.remove('/content/drive/My Drive/analyze_comment_fb/preprocessing/9780.txt')\n",
        "os.remove('/content/drive/My Drive/analyze_comment_fb/preprocessing/9782.txt')\n",
        "os.remove('/content/drive/My Drive/analyze_comment_fb/preprocessing/9784.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBmcoLYBzgq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12124fe3-e1cf-4953-8537-e1f8d087b28d"
      },
      "source": [
        "#import preprocessing\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from underthesea import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "path = '/content/drive/My Drive/analyze_comment_fb/preprocessing/'\n",
        "path_file = os.listdir(path)\n",
        "path_topic = \"/content/drive/My Drive/analyze_comment_fb/topic/\"\n",
        "path_topics = os.listdir(path_topic)\n",
        "path_topics = [e[:-4] for e in path_topics]\n",
        "path_topics = sorted(path_topics,key=len,reverse=True)\n",
        "doc = []\n",
        "label = []\n",
        "for i in path_file:\n",
        "\tp = path+i\n",
        "\twith open(p,\"r\") as f:\n",
        "\t\tdata = f.read()\n",
        "\t\tlink, data = data.split('\\n')[0], data.split('\\n')[1:]\n",
        "\t\tlink = link[23:link.find('/', 23)]\n",
        "\t\tcheck = False\n",
        "\t\tfor tp in range(len(path_topics)):\n",
        "\t\t\tt = path_topics[tp]\n",
        "\t\t\tif link.find(t) != -1:\n",
        "\t\t\t\tlabel.append(tp)\n",
        "\t\t\t\tcheck = True\n",
        "\t\t\t\tbreak\n",
        "\t\tif check == False:\n",
        "\t\t\tprint(p)\n",
        "\t\t\tprint(link)\n",
        "\t\tdata = ' '.join(str(e) for e in data)\n",
        "\t\tdoc.append(data)\n",
        "\t\t#print(link+ \" \"+path_topics[label[-1]])\n",
        "print(str(len(label))+\" \"+str(len(doc)))\n",
        "\n",
        "def diff(list1, list2):\n",
        "\treturn list(set(list1).symmetric_difference(set(list2)))\n",
        "\n",
        "label  = np.array(label)\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "vectorizer_tfidf.fit(doc)\n",
        "X_nmf = vectorizer_tfidf.transform(doc).toarray()\n",
        "# r = [e for e in range(len(label))]\n",
        "# train_ = random.sample(r,int(len(label)*0.8))\n",
        "# X_train , y_train  = np.array([X_nmf[e]  for e in train_]) , np.array([label[e] for e in train_])\n",
        "# r = diff(r,train_)\n",
        "# test_ = random.sample(r,int(len(r)/2))\n",
        "# X_test , y_test  = np.array([X_nmf[e]  for e in test_]) , np.array([label[e] for e in test_])\n",
        "# r = diff(r,test_)\n",
        "# X_validate , y_validate = np.array([X_nmf[e]  for e in r]) , np.array([label[e] for e in r])\n",
        "#sys.exit()\n",
        "X_train , X_test , y_train , y_test = train_test_split(X_nmf,label,test_size=0.2)\n",
        "X_train , X_validate , y_train , y_validate = train_test_split(X_train,y_train,test_size=0.1)\n",
        "\n",
        "model  = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(53,activation='softmax'))\n",
        "model.compile(optimizer='adam',metrics=['acc'],loss='sparse_categorical_crossentropy')\n",
        "model.fit(X_nmf,label,epochs=50,batch_size=64,validation_data=(X_validate,y_validate))\n",
        "model.evaluate(X_test,y_test)\n",
        "# vectorizer_count = CountVectorizer()\n",
        "# X_lda = vectorizer_count.fit_transform(doc)\n",
        "# tf_feature_names = vectorizer_count.get_feature_names()\n",
        "# # Tweak the two parameters below\n",
        "# number_topics = len(path_topics)\n",
        "# # Create and fit the LDA model\n",
        "# lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "# lda.fit(X_lda)\n",
        "# def display_topics(model, feature_names, no_top_words):\n",
        "# \tfor topic_idx, topic in enumerate(model.components_):\n",
        "# \t\tprint(\"Topic %d:\" % (topic_idx))\n",
        "# \t\tprint(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "#\n",
        "# no_top_words = 10\n",
        "# display_topics(lda, tf_feature_names, no_top_words)\n",
        "# #database = lda.fit_transform(X_lda)\n",
        "# #np.save('weights',database)\n",
        "# database = np.load('weights.npy')\n",
        "# print(\"Input file directory : \")\n",
        "# file = input()\n",
        "# with open(file,\"r\") as f:\n",
        "# \tdata = f.read()\n",
        "# \tdata = word_tokenize(data)\n",
        "# \twith open('vietnamese-stopwords.txt','r') as v:\n",
        "# \t\tlist_stopwords = f.read().split('\\n')\n",
        "# \tres = \"\"\n",
        "# \tfor w in data:\n",
        "# \t\tif w not in list_stopwords:\n",
        "# \t\t\tres+=w\n",
        "# \tdata = [res]\n",
        "# \tdata = vectorizer_count.fit_transform(data)\n",
        "# \tresult = lda.fit_transform(data)\n",
        "# min = 9999\n",
        "# topic = \"\"\n",
        "# for i in range(len(doc)):\n",
        "# \ttemp = database[i]\n",
        "# \tdist = np.linalg.norm(temp - result)\n",
        "# \tif dist<min:\n",
        "# \t\tdist = min\n",
        "# \t\ttopic = path_topics[label[i]]\n",
        "# print(\"Your doc topic is : \"+topic)\n",
        "# # check maxlen of doc : 3720\n",
        "# # maxim = 0\n",
        "# # for i in path_file:\n",
        "# # \tp = path+i\n",
        "# # \twith open(p,'r') as f:\n",
        "# # \t\tdata = f.read()\n",
        "# # \t\tlink,data = data.split('\\n')[0],data.split('\\n')[1:]\n",
        "# # \t\tdata = ' '.join(str(e) for e in data)\n",
        "# # \t\tif maxim < len(word_tokenize(data)):\n",
        "# # \t\t\tmaxim = len(word_tokenize(data))\n",
        "# # print(maxim)\n",
        "#\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12862 12862\n",
            "Train on 12862 samples, validate on 1029 samples\n",
            "Epoch 1/50\n",
            "12862/12862 [==============================] - 3s 264us/step - loss: 3.4731 - acc: 0.4894 - val_loss: 3.0558 - val_acc: 0.5724\n",
            "Epoch 2/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 2.7299 - acc: 0.5993 - val_loss: 2.4791 - val_acc: 0.6356\n",
            "Epoch 3/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 2.2295 - acc: 0.6554 - val_loss: 2.0629 - val_acc: 0.6812\n",
            "Epoch 4/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 1.8672 - acc: 0.7116 - val_loss: 1.7549 - val_acc: 0.7337\n",
            "Epoch 5/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 1.5980 - acc: 0.7612 - val_loss: 1.5215 - val_acc: 0.7755\n",
            "Epoch 6/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 1.3923 - acc: 0.8033 - val_loss: 1.3399 - val_acc: 0.8134\n",
            "Epoch 7/50\n",
            "12862/12862 [==============================] - 3s 237us/step - loss: 1.2313 - acc: 0.8300 - val_loss: 1.1954 - val_acc: 0.8348\n",
            "Epoch 8/50\n",
            "12862/12862 [==============================] - 3s 238us/step - loss: 1.1024 - acc: 0.8486 - val_loss: 1.0786 - val_acc: 0.8533\n",
            "Epoch 9/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.9970 - acc: 0.8616 - val_loss: 0.9818 - val_acc: 0.8669\n",
            "Epoch 10/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.9094 - acc: 0.8738 - val_loss: 0.9010 - val_acc: 0.8766\n",
            "Epoch 11/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 0.8355 - acc: 0.8820 - val_loss: 0.8315 - val_acc: 0.8805\n",
            "Epoch 12/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 0.7722 - acc: 0.8877 - val_loss: 0.7724 - val_acc: 0.8902\n",
            "Epoch 13/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.7174 - acc: 0.8949 - val_loss: 0.7207 - val_acc: 0.8921\n",
            "Epoch 14/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.6695 - acc: 0.8997 - val_loss: 0.6753 - val_acc: 0.8980\n",
            "Epoch 15/50\n",
            "12862/12862 [==============================] - 3s 238us/step - loss: 0.6272 - acc: 0.9037 - val_loss: 0.6350 - val_acc: 0.8999\n",
            "Epoch 16/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 0.5895 - acc: 0.9085 - val_loss: 0.5989 - val_acc: 0.9028\n",
            "Epoch 17/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.5558 - acc: 0.9127 - val_loss: 0.5663 - val_acc: 0.9106\n",
            "Epoch 18/50\n",
            "12862/12862 [==============================] - 3s 234us/step - loss: 0.5254 - acc: 0.9148 - val_loss: 0.5371 - val_acc: 0.9077\n",
            "Epoch 19/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.4978 - acc: 0.9170 - val_loss: 0.5102 - val_acc: 0.9106\n",
            "Epoch 20/50\n",
            "12862/12862 [==============================] - 3s 229us/step - loss: 0.4727 - acc: 0.9204 - val_loss: 0.4859 - val_acc: 0.9184\n",
            "Epoch 21/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 0.4497 - acc: 0.9236 - val_loss: 0.4635 - val_acc: 0.9203\n",
            "Epoch 22/50\n",
            "12862/12862 [==============================] - 3s 229us/step - loss: 0.4286 - acc: 0.9259 - val_loss: 0.4430 - val_acc: 0.9232\n",
            "Epoch 23/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.4092 - acc: 0.9290 - val_loss: 0.4237 - val_acc: 0.9291\n",
            "Epoch 24/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3912 - acc: 0.9317 - val_loss: 0.4063 - val_acc: 0.9300\n",
            "Epoch 25/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3746 - acc: 0.9338 - val_loss: 0.3903 - val_acc: 0.9320\n",
            "Epoch 26/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3592 - acc: 0.9365 - val_loss: 0.3749 - val_acc: 0.9329\n",
            "Epoch 27/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3449 - acc: 0.9390 - val_loss: 0.3607 - val_acc: 0.9339\n",
            "Epoch 28/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3316 - acc: 0.9411 - val_loss: 0.3477 - val_acc: 0.9368\n",
            "Epoch 29/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 0.3191 - acc: 0.9438 - val_loss: 0.3355 - val_acc: 0.9378\n",
            "Epoch 30/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.3075 - acc: 0.9456 - val_loss: 0.3239 - val_acc: 0.9407\n",
            "Epoch 31/50\n",
            "12862/12862 [==============================] - 3s 232us/step - loss: 0.2966 - acc: 0.9479 - val_loss: 0.3134 - val_acc: 0.9417\n",
            "Epoch 32/50\n",
            "12862/12862 [==============================] - 3s 228us/step - loss: 0.2864 - acc: 0.9493 - val_loss: 0.3031 - val_acc: 0.9446\n",
            "Epoch 33/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 0.2769 - acc: 0.9506 - val_loss: 0.2937 - val_acc: 0.9485\n",
            "Epoch 34/50\n",
            "12862/12862 [==============================] - 3s 235us/step - loss: 0.2679 - acc: 0.9551 - val_loss: 0.2849 - val_acc: 0.9543\n",
            "Epoch 35/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.2594 - acc: 0.9561 - val_loss: 0.2765 - val_acc: 0.9553\n",
            "Epoch 36/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.2515 - acc: 0.9570 - val_loss: 0.2686 - val_acc: 0.9553\n",
            "Epoch 37/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.2440 - acc: 0.9579 - val_loss: 0.2613 - val_acc: 0.9553\n",
            "Epoch 38/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 0.2369 - acc: 0.9584 - val_loss: 0.2543 - val_acc: 0.9553\n",
            "Epoch 39/50\n",
            "12862/12862 [==============================] - 3s 235us/step - loss: 0.2303 - acc: 0.9591 - val_loss: 0.2478 - val_acc: 0.9553\n",
            "Epoch 40/50\n",
            "12862/12862 [==============================] - 3s 230us/step - loss: 0.2240 - acc: 0.9597 - val_loss: 0.2415 - val_acc: 0.9553\n",
            "Epoch 41/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 0.2181 - acc: 0.9602 - val_loss: 0.2357 - val_acc: 0.9563\n",
            "Epoch 42/50\n",
            "12862/12862 [==============================] - 3s 231us/step - loss: 0.2124 - acc: 0.9607 - val_loss: 0.2302 - val_acc: 0.9572\n",
            "Epoch 43/50\n",
            "12862/12862 [==============================] - 3s 233us/step - loss: 0.2071 - acc: 0.9617 - val_loss: 0.2250 - val_acc: 0.9572\n",
            "Epoch 44/50\n",
            "12862/12862 [==============================] - 3s 229us/step - loss: 0.2021 - acc: 0.9621 - val_loss: 0.2201 - val_acc: 0.9572\n",
            "Epoch 45/50\n",
            "12862/12862 [==============================] - 3s 228us/step - loss: 0.1973 - acc: 0.9624 - val_loss: 0.2156 - val_acc: 0.9572\n",
            "Epoch 46/50\n",
            "12862/12862 [==============================] - 3s 227us/step - loss: 0.1928 - acc: 0.9628 - val_loss: 0.2112 - val_acc: 0.9572\n",
            "Epoch 47/50\n",
            "12862/12862 [==============================] - 3s 229us/step - loss: 0.1885 - acc: 0.9633 - val_loss: 0.2070 - val_acc: 0.9592\n",
            "Epoch 48/50\n",
            "12862/12862 [==============================] - 3s 228us/step - loss: 0.1845 - acc: 0.9636 - val_loss: 0.2031 - val_acc: 0.9602\n",
            "Epoch 49/50\n",
            "12862/12862 [==============================] - 3s 235us/step - loss: 0.1807 - acc: 0.9638 - val_loss: 0.1994 - val_acc: 0.9602\n",
            "Epoch 50/50\n",
            "12862/12862 [==============================] - 3s 264us/step - loss: 0.1770 - acc: 0.9638 - val_loss: 0.1959 - val_acc: 0.9611\n",
            "2573/2573 [==============================] - 1s 195us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15898448344663196, 0.9681305868867488]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhf2TyyG0EWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1f2ca3d9-db80-4047-af23-deb243ce6502"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQlqyKo-0Kna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce468e44-759f-4c07-a7f1-1682898a1a69"
      },
      "source": [
        "\n",
        "data = tokenize('/content/drive/My Drive/analyze_comment_fb/test/test2.txt')\n",
        "with open('/content/drive/My Drive/analyze_comment_fb/vietnamese-stopwords.txt','r') as f:\n",
        "    list_stopwords = f.read().split('\\n')\n",
        "res = \"\"\n",
        "for w in data:\n",
        "    if w not in list_stopwords:\n",
        "        res+=w+\" \"\n",
        "data = [res]\n",
        "X = vectorizer_tfidf.transform(data).toarray()\n",
        "y = model.predict(X)\n",
        "print(path_topics[np.argmax(y)])\n",
        "    "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "an-ninh-hinh-su\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXRQxUiH25uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un3BKyUz2YSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}